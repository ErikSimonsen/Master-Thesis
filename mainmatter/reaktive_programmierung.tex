\section{Reaktive Programmierung}
\label{section:reaktive_programmierung}
Reaktive Programmierung ist ein ereignisgesteuertes (event-driven) Programmierparadigma.
Der Programmablauf besteht aus Sequenzen von asynchronen Ereignissen (Events).
Die Daten werden als von außen unveränderliche (immutable) Datenströme (Streams) dargestellt.
Sobald es innerhalb des Datenstroms zu Änderungen kommt werden diese als Events durch einen Publisher veröffentlicht.
Diese Events werden dann von einem oder mehreren \verb|Subscribern| konsumiert, welche wiederum weitere Events durch einen
Publisher veröffentlichen können.

Die treibende Kraft bzw. die Stimuli von reaktiven Anwendungen sind also interne Änderungen der Datenströme, wie beispielsweise das
Hinzufügen von Elementen.
Diese Änderungen starten dann den weiteren Programmablauf durch das Auslösen eines oder mehrerer Events.

Die Grundidee orientiert sich am Observer-Pattern und dessen Ausprägung, dem Publish-Subscribe Pattern, erweitert dieses aber
noch um die Benachrichtigungen des Subscribers:
\begin{enumerate}
  \item Sobald keine Events mehr kommen
  \item Wenn ein Fehler aufgetreten ist
\end{enumerate}
Indem Änderungen eines Datenstroms direkt propagiert werden und der Subscriber diesen nicht modifizieren kann, sondern lediglich über Änderungen informiert wird,
können Programme ohne jeglichen Zustand realisiert werden\parencite{Escoffier2017}.

Während ein Datenstrom selber von außen unveränderlich ist, kann aber dennoch der Inhalt
der veröffentlichten Events in beliebig vielen Schritten modifiziert werden, bevor der Subscriber diese konsumiert.

Für eine solche Verkettung von Verarbeitungsschritten oft der Begriff \verb|Pipeline| verwendet und jeder Verarbeitungsschritt als \verb|Pipe| bezeichnet.
Durch diese Pipelines \textit{fließen} Elemente in Form von Events bzw. Daten von ihrer Quelle
bis zu einer Senke, dem Subscriber der \verb|Pipeline|.
Daher wird dieser Prozess auch als \verb|event flow| oder \verb|data flow| bezeichnet.
Jede \verb|Pipe| kann ein Element ändern, löschen oder auch neue Elemente erstellen und dem \verb|flow| hinzufügen.

Allgemein fließt ein Element immer stromabwärts, also von der Quelle zur Senke.
\footnote{Es gibt aber auch Ausnahmefälle in denen ein Element stromaufwärts fließt, also von einer pipe oder Senke zur Quelle}
Dabei wird die vorherige \verb|pipe| als \verb|upstream| bezeichnet und die nächste zu durchfließende \verb|pipe| als \verb|downstream|.
Im Hintergrund kommt dabei wieder das Publish-Subscribe Pattern zum Einsatz. Elemente werden von einer durchlaufenen \verb|pipe| als \verb|Publisher|
veröffentlicht und von der nächsten \verb|pipe| als \verb|Subscriber| konsumiert.

Jede \verb|Pipe| kann eine Vielzahl an konkreten Operatoren enthalten die in der jeweiligen Reactive Programming-Bibliothek definiert sind.
Jeder Operator muss dabei wiederum einen Publisher zur Verfügung stellen, auf den sich der nachfolgende Operator registrieren kann
um die transformierten Elementen zu erhalten.\footnote{Obwohl die Implementierungsdetails und die verwendete Terminologie, abhängig von der verwendeten Reactive Programming-Bibliothek
  und der Programmiersprache, unterschiedlich sind ist der Programmablauf immer ähnlich}

In Listing \ref{lst:eventflow_pseudocode} wird der Event-/Datenfluss anhand von Pseudocode dargestellt.
\begin{lstlisting}[caption=Pseudocode Event-/Datenfluss, captionpos=b, label=lst:eventflow_pseudocode]
source <-- source ist der Beginn des streams, also die Quelle
	.operator1() 
	.operator2() 
	.subscribe(consumer)
\end{lstlisting}
\verb|source| ist der Beginn des Streams, also die Quelle. \verb|operator1| nimmt die Elemente des Upstreams \verb|source| entgegen,
modifiziert diese und gibt das Resultat an den Downstream \verb|operator2| weiter.
Dieser tut dasselbe und gibt das Resultat anschließend an den Downstream \verb|subscribe| weiter.
\verb|subscribe| ist das Ende der \verb|Pipeline|, also die Senke, und erlaubt nun das weiterführende Nutzen der erhaltenen Daten.

Abbildung \ref{fig:eventflow_mutiny} zeigt wie der obige, in Pseudocode dargestellte, Datenstrom
mit der Reactive Programming-Bibliothek \verb|Mutiny| aussehen würde.
Die Quelle ist dabei der \verb|Publisher| und \verb|a,b| und \verb|c| die veröffentlichten Events.
\verb|Processors| stellen die Operatoren dar und der \verb|Subscriber| ist die Senke.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.9\textwidth]{EventFlow}
  \caption{Exemplarische Abbildung eines Ereignisflusses in der Reactive Programming-Bibliothek Mutiny \parencite{MutinyEventFlow}}
  \label{fig:eventflow_mutiny}
\end{figure}

Nicht-blockierende, asynchrone Ein- und Ausgabe ist das Grundkonzept reaktiver Programmierung.
Dabei wird nach dem Start der Übertragung bereits begonnen die Teile des Programms auszuführen, die nicht vom Ergebnis der I/O Operation abhängen.
Bei synchroner Ein- und Ausgabe hingegen wird die restliche Ausführung des Programms blockiert
bis die Datenübertragung abgeschlossen ist.
\newpage
\begin{lstlisting}[caption=Pseudocode Non-blocking I/O (NIO), captionpos=b, label=lst:NIO_Pseudocode]
NonBlockingDatabaseRequest().subscribe(print("req finished"))
NonBlockingDatabaseRequest2().subscribe(print("req 2 finished"))
print("hello world")
\end{lstlisting}
In Listing \ref{lst:NIO_Pseudocode} kann durchaus zuerst \verb|hello world| ausgegeben werden da die gestarteten Datenbank-Abfragen die
restliche Programmausführung nicht blockieren. Wenn statt \verb|hello world| versucht werden würde die Ergebnisse der Datenbank-Abfragen
darzustellen, wäre das Ergebnis möglicherweise eine Exception oder \verb|null|, da die Abfragen zum Zeitpunkt der Ausführung von Zeile 3 noch nicht
notwendigerweise abgeschlossen sind. Das ist ein gängiger Fehler beim Arbeiten mit asynchronen, nicht-blockierendem Programmcode.

\subsection{Vorteile und Nachteile}
\label{subsec:vorteile_nachteile}
Die Vorteile der Nutzung von einigen wenigen \verb|Kernel-Threads| als \verb|IO threads| gegenüber eines ganzen Threadpools,
wie beim \verb|Thread per request|-Modell, liegen insbesondere in den Antwortzeiten bei
hoher Last und dem Ressourcenverbrauch.
Umso weniger \verb|Kernel-Threads| gleichzeitig aktiv sind, desto weniger Threadwechsel auf Betriebssystemebene
gibt es auch pro CPU-Kern.

Da Threadwechsel nicht mehr der limitierende Faktor sind und jeder \verb|IO thread| wesentlich mehr CPU-Rechenzeit nutzen kann,
skalieren die Antwortzeiten von Anwendungen wesentlich besser mit der Erhöhung der Last.
Darüber hinaus sinkt auch der allokierte Speicher deutlich
\footnote{Eine 64-Bit JVM reserviert auf Linux-Systemen standardmäßig 1 MiB Speicher für den Threadstack\parencite{OpenJDKGitHub}}
und der Grad der Parallelität wird nicht mehr von der Anzahl der verfügbaren Threads begrenzt.

Die Umstellung auf das reaktive Programmierparadigma erfordert allerdings eine Änderung in der Denk- und Herangehensweise bei der
Entwicklung von Softwarekomponenten. Reaktive Komponenten haben in der Regel keinen Zustand,
sondern reagieren lediglich auf interne Änderungen in den zugrundeliegenden Datenströmen.
Die Programmabläufe müssen daher als reaktive \verb|Pipelines| neu konstruiert werden.

Darüber hinaus ist reaktiver Code schwieriger zu debuggen und zu testen als imperativer Code, denn die Programmlogik die innerhalb der einzelnen Operatoren
der \verb|Pipes| einer \verb|Pipeline| aufgerufen wird, besteht in der Regel aus anonymen Funktionen und ist daher schwierig im Stack Trace zurückzuverfolgen.
Während \verb|Event Flows| bei kleinen Pipelines noch zu überblicken sind, können komplexe Pipelines, die sich über die ganze Anwendung erstrecken,
schnell zu Problemen bezüglich Wartbarkeit und Verständnis führen.
Reaktive Programmierung ist zudem mit einer steilen Lernkurve verbunden. Dies liegt unter anderem an dem aktuellen Interesse an skalierenden
Anwendungen und der damit oftmals einhergehenden falschen Verwendung von Begriffen und der Überladung des Begriffs \verb|Reactive Programming|.
Aber auch die Funktionsweise von asynchronem Code (siehe Listing \ref{lst:NIO_Pseudocode}) kann zu schwer nachvollziehbaren und
reproduzierbaren Fehlern führen. Beispielsweise kann falsch strukturierter Code eine NullPointer-Exception auslösen, weil
auf Daten eines Service zugegriffen wird die zu dem Zeitpunkt bei hoher Latenz noch nicht vorhanden sind, bei niedriger Latenz
allerdings schon.

Außerdem gibt es zum aktuellen Stand kaum allgemeine, herstellerunabhängige Fachliteratur zu dem Thema.
Durch die Vielzahl an lange existierenden Reactive-Programming Bibliotheken\footnote{Diese sind größtenteils bereits vor der Entwicklung der \textit{Reactive Streams}-Spezifikation entstanden}
haben sich darüber hinaus viele verschiedene Nomenklaturen und Vorgehensweisen etabliert.

Ein weiteres Problem besteht bei der Integration in bestehende Enterprise Anwendungen.
Bibliotheken und Konzepte die Themen wie Security, Transaktionen oder Tracing behandeln, führen oft blockierenden, threadspezifischen Code aus.
Jede Schicht einer Anwendung muss reaktiv konzipiert sein, da die Verarbeitung einer Anfrage, die blockierenden Code aus einer der
Schichten aufruft, ansonsten den ausführenden \verb|IO thread| blockieren kann, wodurch dieser keine weiteren Anfragen bearbeiten kann und somit die
gesamte Anwendung blockiert.
\newline\newline
Die wesentlichen Vor-und Nachteile lassen sich wie folgt zusammenfassen:
\newline
\newline
Vorteile
\begin{itemize}
  \item Die Antwortzeiten sind für hohe Lasten geringer, da deutlich weniger Threadwechsel gemacht werden müssen und jeder Thread
        mehr Rechenzeit bekommt
  \item Der Speicherverbrauch ist geringer, da weniger Threads genutzt werden
  \item Der Grad der Parallelität ist nicht von der Anzahl der Threads begrenzt
\end{itemize}
Nachteile
\begin{itemize}
  \item Asynchroner Code ist schwieriger zu schreiben, lesen, testen und zu debuggen als imperativer Code
  \item Sehr aufwendig in bestehende klassische Enterprise-Anwendungen zu integrieren
  \item Reaktive Anwendungen müssen in jeder Schicht reaktiv sein (Transaktionen, Security, Datenbanktreiber)
  \item Blockierende I/O-Operationen führen zur Blockierung eines \verb|IO threads| und damit potentiell zur Blockierung der gesamten Anwendung
\end{itemize}
\newpage
\subsection{Reaktive Datenströme}
\label{section:reaktive_datenströme}
In einer typischen asynchronen Verarbeitungskette von potenziell unbegrenzten Datenströmen,
bestehend aus einem Sender und Empfänger bzw. Publisher und Subscriber, kann es vorkommen,
dass der Sender die Daten schneller an den Empfänger verschickt, als dieser sie verarbeiten kann.
Zwei \verb|naive| Ansätze mit einer Überlastung des Empfängers umzugehen wären:
\begin{enumerate}
  \item Nur der Empfänger reagiert auf eine Überlast. Diese kann sich in einem Speicherüberlauf äußern oder, falls der Puffer des Empfängers eine Größenbeschränkung
        hat, im Verlust der empfangenen Daten
  \item Der Sender begrenzt im Voraus die Datenmenge die er an den Empfänger schickt. Da der Sender allerdings in der Regel nicht weiß wie viel der Empfänger
        verarbeiten kann, sendet er entweder zuviel (es entsteht eine Überlast) oder er sendet zuwenig, wodurch der Durchsatz geringer ist als nötig \parencite{JavaSpektrum2015}
\end{enumerate}
Die Lösung für dieses Problem wird \verb|Back-Pressure| genannt.
Dabei fordert der Empfänger die Daten entsprechend seiner Kapazitäten vom Sender an, wodurch dieser weiß wie viele Daten er maximal versenden darf.
Diese Mitteilung muss asynchron geschehen, da bei einer synchronen Kommunikation die Vorteile der asynchronen, reaktiven Datenverarbeitung
negiert werden.
Da große Anwendungen aus mehreren Schichten (beispielsweise Routing-Schicht, Persistenz-Schicht und Geschäftslogik) bestehen und somit zwischen
dem Sender und Empfänger mehrere Komponenten liegen können, muss jedes
Element der Verarbeitungskette nicht-blockierendes, asynchrones Verhalten implementieren, da ansonsten der Rest der Kette blockiert.

Aus der Intention einen Standard für die asynchrone Verarbeitung von Datenströmen mit nicht-blockierender \verb|Back-Pressure|
zu schaffen, ging die \verb|Reactive Streams|-Initiative hervor.
Innerhalb dieser Initiative haben sich mehrere Arbeitsgruppen gebildet, welche die grundlegenden Semantiken erarbeitet haben und
sie in Form einer eigenen Spezifikation namens \verb|Reactive Streams| und als Interfaces für die JVM veröffentlicht haben.\parencite{ReactiveStreams}
Diese Interfaces wurden anschließend in Java 9 in Form der \verb|Flow-API| der Standardbibliothek hinzugefügt.
Die \verb|Flow-API| stellt Interfaces zur Verfügung mit denen eine
asynchrone, nicht-blockierende Verarbeitung von unbegrenzten Datenströmen durch \verb|Back-Pressure| auf der JVM implementiert werden kann.
\parencite{OracleFlow}.

\subsubsection{Java Flow-API}
\label{subsection:java_flow_api}
Die Flow-API fügt die nicht instanziierbare Klasse \verb|java.util.concurrent.Flow| zur Standardbibliothek hinzu.
Sie enthält 4 Interfaces um das, vom \verb|Reactive Streams|-Projekt spezifizierte, beschriebene Publisher-Subscriber Modell des Eventflusses
reaktiver Anwendungen (siehe Listing \ref{lst:eventflow_pseudocode} und Abbildung \ref{fig:eventflow_mutiny}) mit \verb|Back-Pressure| auszudrücken:

\begin{enumerate}
  \item Publisher
  \item Subscriber
  \item Subscription
  \item Processor
\end{enumerate}

Die \verb|Flow|-Klasse erlaubt es Komponenten zu implementieren, die Teil von reaktiven Pipelines sein können, in denen
\verb|Publisher| Elemente produzieren, die von einem oder mehreren \verb|Subscribern| konsumiert werden.
Die Beziehung zwischen einem \verb|Publisher| und \verb|Subscriber| wird durch eine \verb|Subscription| abgebildet.
Während ein \verb|Publisher| theoretisch eine unbegrenzte Menge an Events liefern kann, wird er durch den \verb|Back-Pressure|-Mechanismus eingeschränkt.
Dadurch liefert der \verb|Publisher| immer nur so viele Elemente wie vom \verb|Subscriber| gefordert.
Der \verb|Publisher| erlaubt einem \verb|Subscriber| sich bei ihm zu registrieren um über die herausgegebenen Events informiert zu werden.
Die Kontrolle über den Fluss an Elementen (flow control), inklusive \verb|Back-Pressure|, zwischen einem \verb|Publisher| und einem \verb|Subscriber|
wird von einer \verb|Subscription| verwaltet.
In Listing \ref{lst:java_flowapi} werden die 4 Interfaces der Flow-API dargestellt:
\begin{lstlisting}[language=java, caption=Die Klasse java.util.concurrent.Flow, captionpos=b, label=lst:java_flowapi]
@FunctionalInterface
public static interface Flow.Publisher<T> {
	public void subscribe( Flow.Subscriber<? super T> subscriber );
}
public static interface Flow.Subscriber<T> {
	public void onSubscribe( Flow.Subscription subscription );
	public void onNext( T item );
	public void onError( Throwable throwable );
	public void onComplete();
}
public static interface Flow.Subscription {
	public void request( long n );
	public void cancel();
}
public static interface Flow.Processor<T,R>
extends Flow.Subscriber<T>, Flow.Publisher<R> {
}
\end{lstlisting}\parencite[Kapitel 5.11]{JavaSE9StandardBibliothek}

Die vier \gls{callback}(*)-Methoden des \verb|Subscriber|-Interface werden vom \verb|Publisher| aufgerufen sobald eines der jeweiligen Events ausgelöst wird.
Die Events müssen dabei immer in der gleichen Reihenfolge veröffentlicht, und die jeweiligen Callback-Methoden ausgeführt, werden:
\begin{enumerate}
  \item onSubscribe
  \item onNext*
  \item (onError | onComplete)?
\end{enumerate}
Die Notation bedeutet, dass \verb|onSubscribe| immer als erstes aufgerufen werden muss, gefolgt von einer beliebigen Anzahl an
\verb|onNext|-Aufrufen. Dieser Strom an Events kann theoretisch ewig weitergehen oder durch ein \verb|onComplete|-Aufruf, welcher
signalisiert das keine weiteren Elemente mehr vom \verb|Publisher| produziert werden, beendet werden.
Im Fehlerfall wird vom \verb|Publisher| ein Error-Event veröffentlicht, und das \verb|onError|-Callback des Subscribers aufgerufen.

Sobald sich ein \verb|Subscriber| bei einem \verb|Publisher| registriert wird dessen \verb|onSubscribe|-Methode mit einem neuen
\verb|Subscription|-Objekt aufgerufen. Ein \verb|Subscription|-Objekt wird nur von genau einem \verb|Subscriber| und einem \verb|Publisher| genutzt
und bildet die einzigartige Beziehung zwischen ihnen ab.

Der \verb|Subscriber| kann die \verb|request|-Methode des \verb|Subscription|-Interfaces nutzen um den \verb|Publisher| zu informieren das er bereit
ist eine gegebene Anzahl an Events zu verarbeiten (Back-Pressure). Mit der \verb|cancel|-Methode kann er
die \verb|Subscription| abbrechen und dem \verb|Publisher|
somit mitteilen das er nicht länger Events erhalten will.
\parencite{OracleFlow}

Das \verb|Processor|-Interface erweitert \verb|Publisher| und \verb|Subscriber| ohne das weitere Methoden implementiert werden müssen.
Dieses Interface repräsentiert eine Transformation der Events die durch den
reaktiven Datenstrom produziert werden (siehe Listing \ref{lst:eventflow_pseudocode}).
Klassen die dieses Interface implementieren repräsentieren in der Regel die Operatoren einer Reactive Programming-Bibliothek.
Sobald ein \verb|Processor| ein Fehler erhält kann er sich entweder davon erholen oder direkt ein Error-Event an seinen
Downstream-Subscriber propagieren. Ein typischer Lebenszyklus eines reaktiven Datenstroms mit der Flow-API wird in
Abbildung \ref{fig:flow-api} gezeigt.
\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.9\textwidth]{flow-api_manning.PNG}
  \caption{Lebenszyklus eines reactive streams mit der Flow-API \parencite[Kapitel 17,  Figure 17.3]{JavaInAction}}
  \label{fig:flow-api}
\end{figure}
\newpage

\subsection{Reaktive Systeme}
\label{subsection:reaktive_systeme}
Anforderungen an große Softwaresysteme haben sich in den letzten Jahren stark verändert:
\begin{itemize}
  \item Antwortzeiten in Millisekunden statt im Sekundenbereich
  \item Datengrößen in Petabytes statt Gigabytes
  \item 100\% Verfügbarkeit statt stundenlange Wartungsarbeiten
  \item Deployment auf einer Vielzahl von Plattformen und cloud-basierten Clustern mit tausenden Mehrkernprozessoren
\end{itemize}

Unternehmen aus verschiedenen Bereichen haben sich voneinander unabhängig an diese Kriterien angepasst und Architekturmuster
erarbeitet, mit denen robuste, belastbare und flexible Softwaresysteme entwickelt werden können, welche diese modernen Anforderungen
erfüllen.

2014 wurde mit dem \verb|Reactive Manifesto| versucht, diese Ansätze der Systemarchitektur in Form eines Manifests zusammenzuführen
und daraus allgemeingültige Systemattribute abzuleiten.

\subsubsection{Eigenschaften}
\label{subsubsec:reaktive_systeme_eigenschaften}
Laut des \verb|Reactive Manifesto| sind Systeme reaktiv wenn sie folgende Eigenschaften besitzen:
\begin{itemize}
  \item Reaktionsschnell
  \item Widerstandsfähig (gegen Fehler)
  \item Elastisch
  \item Nachrichtengesteuert
\end{itemize}
Solche Systeme sind, laut den Autoren, flexibler, stärker entkoppelt und besser skalierbar als herkömmliche, nicht-reaktive Systeme.
Dies mache sie leichter zu entwickeln, zugänglicher für Veränderungen und deutlich fehlertoleranter.
Die Autoren definieren die genannten Systemeigenschaften wie folgt:
\paragraph{Antwortbereit}Das System reagiert, falls überhaupt möglich, zeitgerecht. Antwortbereitschaft ist dabei die Grundlage für Funktion und
Benutzbarkeit. Es ermöglicht das schnelle Erkennen und Behandeln von Fehlern, indem verlässliche, zeitliche Obergrenzen für die Antworten von
Komponenten innerhalb des Systems geschaffen werden. Sobald eine Komponente nicht innerhalb des Zeitfensters antwortet, wird dies als Fehler gewertet und kann
entsprechend behandelt werden.
Der Fokus liegt auf konsistenten und schnellen Antwortzeiten. Darüber hinaus schaffen sie
verlässliche Obergrenzen um eine konsistente Qualität zu erreichen.
Dieses Verhalten vereinfacht Fehlerbehandlung, und erhöht das Vertrauen der Benutzer bezüglich der Interaktion.

\paragraph{Widerstandsfähig/Fehlertolerant}Das System bleibt auch bei Fehlern oder Ausfällen antwortbereit.
Das gilt nicht nur für geschäftskritische, hochverfügbare Systeme -
jedes System das nicht widerstandsfähig ist, wird nach Fehlern nicht mehr antwortbereit und damit funktionsuntüchtig sein.
Widerstandsfähigkeit wird durch Replikation von Funktionalität (Redundanz), Eingrenzung von Fehlern, Isolation von Komponenten und
Delegation von Verantwortung realisierbar.
Fehler werden innerhalb einer Komponente eingegrenzt und die Komponenten sind voneinander isoliert. Dadurch bleibt das Gesamtsystem stabil, selbst
wenn eine einzelne Komponente versagt.
Die Wiederherstellung jeder Komponente (self-healing) wird an eine andere, möglicherweise externe, Komponente delegiert.
Die Hochverfügbarkeit der Komponenten wird, wo notwendig, durch Redundanz gewährleistet.

\paragraph{Elastisch}Das System bleibt reaktionsschnell unter variierenden Arbeitslasten. Auf Änderungen der Arbeitslast wird durch
Anpassung der allokierten Ressourcen, insbesondere Replikation, reagiert. Das impliziert ein Systemdesign das keine zentralen Bottlenecks oder
Reibungspunkte hat, damit Komponenten problemlos repliziert und die Last darauf verteilt werden kann.
Reaktive Systeme unterstützen prädiktive, skalierende Algorithmen zur Ressourcenberechnung,
indem sie die Auslastung zur Laufzeit erfassen und als Eingabe nutzen.

\paragraph{Nachrichtenorientiert}Reaktive Systeme basieren auf asynchronem,
nicht-blockierendem Austausch von Nachrichten, um die Komponenten voneinander
abzugrenzen und dadurch eine lose Kopplung, Isolation und eine transparente Lokalisierung der Komponenten zu ermöglichen.
Aufgrund dieser Abgrenzung werden Fehler als Nachrichten an andere Komponenten delegiert.
Der Ansatz, jegliche Kommunikation der Komponenten durch das Übermitteln von Nachrichten zu implementieren, ermöglicht Elastizität,
indem er das Verteilen der Arbeitslast, die Kontrolle der Datenströme und, falls nötig, Anwenden von \verb|Back-Pressure| erlaubt.
Die Kontrolle der Datenströme erfolgt durch das Überwachen der Nachrichtenwarteschlangen zur Laufzeit (\textit{message queues}).

Ortsunabhängigkeit bedeutet, dass Code und Semantik des Programms nicht davon abhängen, ob dessen Teile auf demselben Computer
oder verteilt über ein Netzwerk ausgeführt werden.
Reaktive nachrichtenorientierte Systeme erlauben eine effiziente Verwendung von Ressourcen, da Komponenten beim Ausbleiben von
Nachrichten vollständig inaktiv bleiben können, und sich nicht regelmäßig über neue Nachrichten informieren müssen.\parencite{ReactiveSystems}

Im Gegensatz zu Events haben Nachrichten immer ein klar definiertes Ziel.
Das bedeutet, dass sich ein ereignisgesteuertes System- oder eine Komponente auf adressierbare Event-Quellen konzentriert,
während ein nachrichtenorientiertes System auf adressierbaren Empfängern basiert.

In Abbildung \ref{fig:reactive-traits} wird das Zusammenspiel der Eigenschaften eines reaktiven Systems dargestellt.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.9\textwidth]{reactive-traits-de}
  \caption{Zusammenspiel der Eigenschaften eines reaktiven Systems \parencite{ReactiveSystems}}
  \label{fig:reactive-traits}
\end{figure}
\newpage
\subsubsection{Abgrenzung zu reaktiver Programmierung}
\label{subsubsection:abgrenzung_reaktive_programmierung}
Aufgrund der steigenden Popularität von reaktiven Anwendungen ist der Begriff \verb|reaktiv| im Kontext der Softwareentwicklung
überladen und unterscheidet nicht zwischen \newline\verb|Reaktiver Programmierung| und \verb|Reaktiven Systemen|.

\verb|Reaktive Programmierung| ist eine ideale Technik zur Abbildung der internen Logik innerhalb einer Komponente in Form von Transformationen
auf Datenströmen, um sowohl Leistungsfähigkeit, Ressourceneffizienz und Code-Verständlichkeit zu optimieren.

\verb|Reaktive Systeme| sind hingegen eine Menge von architektonischen Prinzipien, welche die verteilte Kommunikation hervorheben und
Ansätze zur Realisierung von Fehlertoleranz und Elastizität liefern.

Ein gängiges Problem bei ausschließlicher Nutzung von \verb|Reaktiver Programmierung| besteht darin, dass die enge Kopplung
zwischen Verarbeitungsschritten (Transformationen) in einem ereignisgesteuerten Programm es erschwert die geforderte Fehlertoleranz
eines reaktiven Systems zu erreichen.
Die \verb|Pipelines| sind kurzlebig und die Operatoren und Callback-Methoden innerhalb der \verb|Pipes|
sind anonym, also nicht adressierbar.

Das bedeutet, dass sowohl Erfolg als auch Fehler direkt behandelt werden, ohne andere Komponenten darüber zu informieren.
Dieser Mangel an Adressierbarkeit erschwert die Wiederherstellung einzelner Phasen, da unklar ist, wo beziehungsweise ob Ausnahmen
propagiert werden sollten. Infolgedessen sind Fehler an kurzlebige Client-Anfragen und nicht an den
Gesamtzustand der Komponente gebunden – wenn eine der Phasen in der \verb|Pipeline| fehlschlägt, muss die gesamte \verb|Pipeline| neu
gestartet und der Client benachrichtigt werden. Dies steht im Gegensatz zu einem nachrichtengesteuerten reaktiven System, das
die Fähigkeit zur Wiederherstellung von Komponenten besitzt, ohne dass der Client benachrichtigt werden muss.

Ein weiterer Kontrast zum Ansatz eines reaktiven Systems ist, dass rein reaktive Programmierung zwar eine zeitliche Entkopplung,
aber keine räumliche Entkopplung ermöglicht. Zeitliche Entkopplung erlaubt die Kommunikation von Komponenten, ohne dass sie gleichzeitig
verfügbar sein müssen, während räumliche Entkopplung
die Verteilung auf mehrere Systemkomponenten erlaubt. Dadurch können nicht nur statische, sondern auch dynamische Topologien
realisiert werden, was für die Elastizität eines reaktiven Systems essentiell ist.

Insgesamt ist \verb|reaktives Programmieren| eine sehr nützliche Technik, die in einer reaktiven Architektur genutzt werden kann.
Das Implementieren von Datenflüssen durch asynchrone und nicht-blockierende Ausführung innerhalb eines Services kann die
Basis eines reaktiven Systems bilden, mehrere reaktive Services bilden aber nicht direkt ein reaktives System.\parencite{Lightbend}

Sobald mehrere Services in einem solchen System miteinander arbeiten sollen, müssen unter anderem Funktionalitäten wie
Datenkonsistenz, serviceübergreifende Kommunikation, Versionierung, Orchestrierung, Fehlermanagement und Trennung von Verantwortlichkeiten
berücksichtigt werden.

\subsection{Java Ökosystem}
\label{subsec:java_ökosystem}
Im Java Ökosystem gibt es eine Vielzahl an Libraries und Frameworks mit denen
\verb|Reaktive Programmierung| und \verb|Reaktive Systeme| umgesetzt werden können.
Um in Java einzelne, asynchrone Prozesse zu implementieren, wird vom JDK die Future-API zur Verfügung gestellt.\parencite{OracleFuture}
Für die Verarbeitung von asynchronen unbegrenzten Datenströmen gibt es die Interfaces der Flow-API (siehe Kapitel \ref{subsection:java_flow_api}).
\parencite{OracleFlow}
Im Folgenden wird ein Überblick über die Vielzahl an Bibliotheken und Frameworks gegeben.
\subsubsection{Reaktive Datenströme - Bibliotheken}
\label{subsubsec:reactive_streams}
Da die Flow-API lediglich Interfaces bereitstellt, gibt es mehrere Implementierungen von \verb|Reaktiven Datenströmen|.
Jedes Projekt unterscheidet sich dabei in den verwendeten Klassen und Operatoren um die \verb|Flow-API| des JDK, und damit auch \verb|Back-Pressure|,
zu implementieren. Trotzdem sind sie interoperabel und bieten in der Regel Converter-Klassen an.
Nachfolgend werden einige der populärsten Bibliotheken inklusive der dazugehörigen APIs aufgelistet:
\begin{itemize}
  \item RxJava\footnote{Implementiert die Flow-API erst seit Version 2} - Flowable, Observable \parencite{RxJava}
  \item Project Reactor - Flux, Mono \parencite{ProjectReactor}
  \item Mutiny - Multi, Uni \parencite{Mutiny}
\end{itemize}

\subsubsection{Reaktive Systeme - Toolkits}
\label{subsubsec:reaktive_systeme}
Für die Entwicklung von reaktiven Systemen bieten sich mehrere Toolkits an.
sie implementieren bereits Mechanismen wie Messaging, Event Loops,
nicht-blockierende Netzwerkkomponenten und Dateizugriffe, sowie umfangreiche Funktionen für Webanwendungen.
Zu den populärsten gehören:
\begin{itemize}
  \item Eclipse Vert.x \parencite{Vert.x}
  \item Akka \parencite{Akka}
  \item Reactor-Netty \parencite{ProjectReactor}
\end{itemize}
Sowohl \verb|Eclipse Vert.x| als auch Reactor-Netty nutzen dabei \verb|Netty|
(siehe Kapitel \ref{subsubsec:reactor_pattern}) als Low-Level Framework für \verb|Non-blocking I/O|.
Da das in dieser Arbeit verwendete Anwendungsframework \verb|Quarkus| auf \verb|Eclipse Vert.x| basiert, wird Vert.x im Folgenden genauer beschrieben:

\paragraph{Eclipse Vert.x}
Vert.x ist ein Toolkit für das Entwickeln von reaktiven Anwendungen auf der JVM und wird durch die herstellerneutrale Eclipse Foundation entwickelt.
Da es sich nicht um ein Framework handelt, gibt es keine vorausgesetzte Anwendungsstruktur und dadurch kann es auch in bestehende Projekte
integriert werden.
Der Kern des Toolkits \verb|vertx-core| stellt APIs für asynchrones Programmieren, \verb|Non-blocking I/O|, Streaming und Zugriff zu
Netzwerkprotokollen wie TCP, UDP, DNS, HTTP oder WebSockets zur Verfügung.
\begin{figure}[ht!]
  \centering
  \includegraphics[width=1.0\textwidth]{vertx}
  \caption{Vert.x Struktur \parencite{Ponge2020}}
  \label{fig:vertx}
\end{figure}
Für die räumliche Entkopplung eines reaktiven Systems bietet Vert.x einen \verb|Event Bus|.
Der \verb|Event Bus| stellt ein verteiltes Peer-To-Peer Nachrichtensystem dar, welches sich über mehrere Serverknoten erstreckt.
Damit können verschiedene Komponenten eines Systems, unabhängig von der genutzten Programmiersprache, durch eindeutige Adressierung miteinander kommunizieren.
Auf jede Adresse können sich, ganz nach dem \verb|Publish-Subscribe|-Modell, mehrere Subscriber registrieren. Sobald Nachrichten an eine
Adresse veröffentlicht werden, werden diese an jeden \verb|Subscriber| weitergeleitet.

Für die Resilienz eines reaktiven Systems verfügt Vert.x über einen Hochverfügbarkeits-Modus (high availablity, HA).
Dabei wird im Falle des Versagens einer Vert.x-Instanz die Anwendung auf eine andere Instanz innerhalb eines Clusters redeployed.
Darüber hinaus wird beim Ausfall einer Komponente durch einen \Gls{circuitBreaker}(*) vermieden, dass weitere Anfragen an diese
Komponente getätigt werden.

Eine Implementierung des \verb|Reactor|-Pattern nutzt standardmäßig einen einzigen \linebreak\verb|IO thread|,
der eine \verb|Event loop| ausführt.
Da durch einen einzigen \verb|IO thread| allerdings zu jeder Zeit auch nur ein CPU-Kern genutzt wird, kann
eine solche Implementierung Mehrkern-CPUs nur nutzen, indem mehrere Prozesse gestartet und verwaltet werden.

Statt einer einzigen Event Loop hält jede Vertx-Instanz, abhängig von der Anzahl der verfügbaren CPU-Kerne\footnote{1 Thread pro logischem CPU-Kern},
mehrere \verb|IO threads|. Diese Threads werden aus einem im vornherein erzeugten \verb|Thread pool| entnommen.
\footnote{Die Größe des \textit{Thread pools} kann auch manuell überschrieben werden}

Um dieses Pattern vom Single Threaded Reactor-Pattern zu unterscheiden, wird es in der Vert.x-Dokumentation als \verb|Multi-Reactor Pattern| bezeichnet.
\parencite{Vert.xDocs}

Im Falle, dass die Event Loop eines \verb|IO threads| durch \verb|Blocking I/O| blockiert wird, wird die Operation stattdessen auf einem
\verb|Worker thread| (siehe Kapitel \ref{subsubsec:thread per request}), aus einen im vornherein erzeugten \verb|Worker thread pool|, abgewickelt.
Diesen Prozess bezeichnet \verb|Vert.x| als \verb|Dispatching|.
Durch \verb|Dispatching| entstehen unvermeidbare Threadwechsel, aber es ermöglicht die Nutzung von Vert.x in bestehenden Projekten, die nicht komplett
auf \verb|Non-blocking I/O| basieren.\parencite[Seite 2]{VertxArticle}

Vert.x führt also zwei Thread Pools mit \verb|Kernel-Threads|, einen für \verb|IO threads|, dessen Größe der Anzahl der logischen CPUs entspricht,
und einen für \verb|Worker threads|, auf dem die Operationen abgearbeitet werden, welche die \verb|IO threads| blockieren würden.
Der \verb|Thread pool| aus \verb|IO threads| wird auch als \verb|Event loop thread pool| bezeichnet.
\parencite{Vert.xOptions}

\subsubsection{Frameworks}
\label{subsubsec:frameworks}
Um komplette Web-Applikationen zu implementieren bietet sich der Gebrauch von Frameworks an, die Entwicklern viele grundlegende Architekturentscheidungen
abnehmen und bereits vorgefertigte Lösungen für Themen wie Authentifizierung, Routing, \Gls{ormg}(*), Security und Serialisierung anbieten oder
Lösungen von Drittanbietern integrieren und vorkonfigurieren.
Um reaktive Anwendungen und Systeme mit mehreren Services sinnvoll zu nutzen, ist es notwendig dass jede, für die Abarbeitung eines Requests relevante,
Anwendungsschicht und Komponente auch reaktiv ist und \verb|Non-blocking I/O| benutzt, sowie eine API bietet auf der
sich Event-Handler beziehungsweise Subscriber registrieren können.\newline
Die populärsten Frameworks mit einem komplett reaktiven Stack sind dabei:

\begin{itemize}
  \item Spring WebFlux
  \item Quarkus
\end{itemize}

Spring WebFlux nutzt dabei \verb|Project Reactor| und Quarkus \verb|Vert.x| als reaktive Engine. \parencite{QuarkusReactiveGettingStarted}
Da die exemplarischen Anwendungen dieser Arbeit (siehe Kapitel \ref{section:vergleich_reaktiv_blockierend}) das Quarkus-Framework nutzen, wird dies
im Folgenden genauer beschrieben:
\paragraph{Quarkus und native image}

Bei dem, in dieser Arbeit verwendeten, Framework \verb|Quarkus| handelt es sich laut Hersteller \textit{Red Hat} um ein
benutzerfreundliches, auf Entwickler abgestimmtes Java Framework, welches für Container-, Cloud- und Serverless-Umgebungen optimiert ist und nur wenig
Konfiguration benötigt. Dabei werden nur die besten und hochwertigsten Java-Bibliotheken und Standards verwendet.
Dabei können die Anwendungen sowohl auf einer JVM (JVM mode) laufen, als auch, durch native Kompilierung mit vollständigem Stack,
als nativ ausführbare Anwendung: dem \verb|native image| (native mode).

Dafür wird eine von Oracle entwickelte Technologie names \verb|GraalVM| genutzt.
Dabei handelt es sich um eine polyglotte, virtuelle Maschine und Laufzeitumgebung die auf dem OpenJDK basiert, und über
JVMCI \footnote{Java Virtual Machine Compiler Interface} den C2-Compiler der zugrundeliegenden HotSpot-JVM durch den
polyglotten Graal Compiler ersetzt.\parencite{GraalVM}

Beim C2-Compiler handelt es sich um einen aggressiv optimierenden \verb|just-in-time|-Compiler (JIT) für Serveranwendungen, bei denen es nicht auf
schnelle Startzeiten und geringen Ressourcenverbrauch, sondern auf höchsten Durchsatz ankommt.
Dabei wird der unoptimierte Bytecode während der Laufzeit in Maschinencode übersetzt und ausgeführt. Sobald eine Methode hinreichend oft
ausgeführt wurde, daher auch als \verb|JIT Warm up| bezeichnet, und der Profiler genügend Informationen gesammelt hat, kann der JIT-Compiler
sie entsprechend optimieren bevor sie in Maschinencode übersetzt wird.
Der Graal Compiler kann als JIT-Compiler im JVM mode sowie als \verb|ahead-of-time|-Compiler (AOT) im native mode genutzt werden.\newline

Um ein \verb|native image| zu erstellen wird mithilfe des Graal Compilers vor dem Kompilieren der Anwendung, also \verb|ahead-of-time|,
analysiert, welche möglichen Pfade das Programm bei einem gegebenen Klassenpfad durchlaufen kann. Da dadurch nur die tatsächlich benötigten Klassen
kompiliert werden, sind die resultierenden Kompilate, um einige Größenordnungen kleiner als die Summe aus JDK und den benötigten Bibliotheken.
Allerdings dauert der Vorgang aufgrund der umfassenden Analyse auch wesentlich länger als die Übersetzung in Bytecode mit \verb|javac|.
Damit Entwickler sich bei Java Anwendungen auch weiterhin nicht um Speicher- und Threadverwaltung kümmern müssen, wird die \verb|SubstrateVM|, eine
leichtgewichtige virtuelle Maschine, in das \verb|native image| hineinkompiliert.
Sie stellt Laufzeitkomponenten wie den Garbage Collector und den Thread Scheduler bereit.

Die Analyse vor dem Kompilieren erfolgt unter der \verb|closed world|-Annahme. Dabei wird angenommen das jeglicher Code der zur Laufzeit des Programms
erreichbar sein soll zur Build-Time des \verb|native image| bekannt sein muss. Dadurch verursachen jegliche Java-Features die erst zur Laufzeit
auswertbar sind, wie beispielsweise Reflection oder statische Initialisierung mit Datumswerten, Probleme für die AOT-Analyse.

Um dem Compiler die notwendigen Informationen zur Unterstützung von Reflection mitzuteilen, können
diese über Konfigurationsdateien oder programmatisch, über die GraalVM- und SubstrateVM-API hinterlegt werden.
Dadurch können auch Anwendungen und Libraries, wie beispielsweise Hibernate, Netty oder Tomcat,
die starken Gebrauch von Reflection machen, so angepasst werden, dass sie in einem \verb|native image| genutzt werden können.
\parencite{GraalVMNativeImage}

Quarkus verspricht durch \verb|native images| bis zu 300 Mal schnellere Startzeiten
und nur ein Zehntel des Speicherbedarfs im Vergleich zu traditionellen Java-Frameworks wie Spring Boot.
Dadurch stellt ein \verb|native image| eine signifikante Reduzierung der benötigten Ressourcen und Kosten
im Container-, Cloud- und Serverless-Umfeld gegenüber einer Anwendung auf der JVM dar.
\parencite{RedHatQuarkusInfografik}

Allerdings ist der maximal mögliche Durchsatz (noch) deutlich niedriger gegenüber Anwendungen im \verb|JVM mode|, da durch die AOT-Kompilierung
auf adaptive Laufzeitoptimierungen wie JIT-Compiling verzichtet wird.
Der Leiter des GraalVM-Projektes hat allerdings in einer Diskussion auf GitHub erwähnt, dass es durch Profile Guided Optimizations (PGO)
theoretisch durchaus möglich sei, einen gleichwertigen Durchsatz zu erreichen\parencite{GraalWuerthinger}.

Bei PGO handelt es sich, wie bei JIT-Compiling auch, um Profiling-Daten die zur Laufzeit des Programms gesammelt werden.
Im Gegensatz zu einer JVM-Anwendung liegt ein native image jedoch als Maschinencode und nicht als Bytecode vor,
und kann daher zur Laufzeit nicht optimiert werden.

Aus diesem Grund wird zuerst ein erstes \verb|native image| generiert und ausgeführt.
Die hierbei ermittelten Profiling-Daten werden anschließend der Generierung des zweiten \verb|native image| als Parameter hinzugefügt,
wodurch die Optimierungen bereits während des Build-Vorgangs ausgeführt werden können.\parencite{GraalVMNativeImagePGO}\newline

Des Weiteren erlaubt Quarkus die Kombination von blockierendem und nicht blockierendem, reaktiven Code.
Dabei wird der Dispatching-Mechanismus von \verb|Vert.x| genutzt (siehe Kapitel \ref{subsubsec:reaktive_systeme}).
Sobald also eine \verb|Blocking I/O|-Operation aufgerufen wird, wird diese auf einem \verb|Worker thread| abgewickelt,
anstatt die \verb|Event Loop|, und damit den
gesamten \verb|IO thread|, zu blockieren.
Für die reaktive Programmierung bietet Quarkus die bereits genannte Bibliothek \verb|Mutiny|
(siehe Kapitel \ref{subsubsec:reactive_streams}) an.\parencite{Quarkus}

\subsection{Alternativen}
\label{subsec:alternativen}
In Java 1.1 wurden Threads als sogenannte \verb|Green threads| implementiert.
Dabei wurde die Möglichkeit, Threads vom Betriebssystem verwalten zu lassen, also \verb|Kernel-Threads|,
gar nicht genutzt.
\verb|Green threads| waren stattdessen als \verb|User-Threads| implementiert,
dabei ist die Funktionalität
nicht im Kernel implementiert, sondern in einer Programmbibliothek im Benutzeradressraum (siehe Kapitel \ref{subsubsec:user-threads}).
Da sich das Betriebssystem nicht um das Scheduling von \verb|User-Threads| kümmert, wurde dies über einen eigenen
Scheduling-Algorithmus der JVM geregelt.\parencite{Oracle2010}
Ein \verb|Green thread| existiert lediglich als Objekt innerhalb der JVM, und durch die virtuelle Speicherverwaltung entfallen
somit die kostenintensiven Betriebssystemaufrufe beim
Erstellen eines Threads sowie bei Threadwechseln.
Die Threadwechsel der \verb|Green threads| erfolgten ausschließlich innerhalb des Main-Threads, weswegen keine echte Parallelität
realisiert werden konnte, da immer nur ein Prozessorkern genutzt wurde.

Während der Vorteil dieses Modells darin lag, dass es keine 'echten' parallelen Zugriffe auf eine Resource innerhalb des Anwendungs-Prozesses geben konnte
und die Synchronisation von Datenzugriffen daher leicht war, überwog schließlich der Umstand, dass eine Nutzung von mehreren Prozessorkernen
durch Multithreading nicht möglich war. Deswegen wurden \verb|Green threads| ab Java 1.3 zugunsten von \verb|Kernel-Threads| entfernt.\newline

Mit dem OpenJDK Projekt \verb|Project Loom| wurde die Idee von \verb|User-Threads| wieder aufgegriffen, allerdings nun als Ergänzung zu \verb|Kernel-Threads|.
Für die konkrete Implementierung der \verb|User-Threads| nutzt das Projekt außerdem die Bezeichnung \verb|virtuelle Threads| statt \verb|Green threads|.
Statt alle virtuellen Threads auf dem Main-Thread auszuführen, werden diese von einer geringen Anzahl an
\verb|Kernel-Threads| ausgeführt, die als \verb|Carrier|\footnote{Ein Kernel-Thread der mehrere User-Threads ausführt} eingesetzt werden.
Deren Anzahl ist so gewählt, dass alle CPU Kerne durch Multithreading dauerhaft benutzt werden
können um so wenig Kontextwechsel wie möglich hervorzurufen.\parencite{Oracle2021}

Während eine 64-Bit JVM für einen \verb|Kernel-Thread| auf Linux-Systemen standardmäßig 1 MiB für den Threadstack reserviert\parencite{OpenJDKGitHub},
und zusätzlich noch Metadaten abspeichert, ist ein virtueller Thread
lediglich ein Objekt im virtuellen Speicher der JVM, weswegen er sehr wenig Ressourcen benötigt.
Aus diesem Grund können, bei entsprechend allokiertem Heap-Speicher der JVM, durchaus mehrere Millionen \verb|User-Threads| erzeugt werden.\newline

In Kapitel \ref{subsubsec:user-threads} wurde bereits die Problematik erwähnt, dass ein Kernel-Thread blockiert und somit
das Scheduling der Laufzeitumgebung nicht ausführen kann, wenn ein darüber liegender \verb|User-Thread| eine blockierende I/O-Operation ausführt.
Dies würde bei mehreren \verb|Kernel-Threads| wieder zu Threadwechseln seitens des Kernel-Schedulers führen, um nicht den gesamten Anwendungsprozesses
zu blockieren.
Die vorgeschlagene Lösung besteht in der Nutzung einer asynchronen, nicht-blockierenden API für I/O Operationen von der Laufzeitumgebung.
Dadurch erhält der Aufrufer, in diesem Fall der \verb|Carrier|, nach dem Start einer blockierenden I/O-Operation
innerhalb eines virtuellen Threads unverzüglich die Kontrolle zurück und kann somit einen weiteren virtuellen Thread bearbeiten.

\verb|Project Looms| Absicht ist es allerdings eine Lösung zu finden die \textbf{ohne} asynchrone APIs für I/O-Operationen auskommt.
Statt einen asynchrone I/O-Operation auszuführen gibt ein virtueller Thread freiwillig die Kontrolle an den Carrier ab (kooperatives Multitasking),
wenn er auf I/O wartet und dementsprechend pausiert. Dadurch blockiert er den Carrier nicht, der den virtuellen Thread ausführt.
Der Carrier führt dann durch einen virtuellen Threadwechsel den nächsten virtuellen Thread aus.

Das Grundkonzept für die Implementierung von solchen virtuellen Threads wird als \verb|Delimited Continuation| oder \verb|Coroutine| bezeichnet.
Eine \verb|Delimited Continuation| ist eine Folge von Anweisungen, die unterbrochen und später vom Aufrufer fortgesetzt werden kann.
Beim Unterbrechen durch die \verb|yield|-Methode der Continuation wird der aktuelle Zustand des ausführenden Threads
(Stack, Registerinhalte und Programmzähler) in eine Funktion umgewandelt.
Diese Funktion wird innerhalb der Continuation gespeichert, und die Kontrolle wird zurück an den Aufrufer gegeben.
Der Aufrufer kann nun zu einem späteren Zeitpunkt einen virtuellen Threadwechsel ausführen und die \verb|run|-Methode der Continuation aufrufen.
Diese ruft die gespeicherte Funktion auf, und kann somit die Ausführung an der Stelle weiterführen, an der sie unterbrochen wurde.

Damit bieten \verb|Continuations| genau die Funktionalität, die für die Implementierung von virtuellen Threads in \verb|Project Loom| benötigt wird:
Ein Programm zu einem beliebigen Zeitpunkt anzuhalten, und später von diesem Punkt aus fortzusetzen.

Eine \verb|Continuation| ist also ein sehr mächtiges Konstrukt um den Programmfluss beliebig zu beeinflussen.

Um eine hohe Abwärtskompatibilität sowie möglichst geringen Migrationsaufwand für Entwickler zu gewährleisten, werden die Änderungen von
Project Loom in einer eigenen JDK Version bereitgestellt.
In dieser Version sind viele Teile der Standardbibliothek die mit I/O-Operationen arbeiten, so angepasst, dass virtuelle Threads statt Kernel-Threads
genutzt werden. Auf diese Weise können I/O-Operationen, wie der Aufruf einer Netzwerkfunktionalität, die virtuellen Threads ohne Änderungen am Programm nutzen
und blockieren den darunterliegenden \verb|Kernel-Thread| nicht mehr.
Allerdings ist ein Zeitpunkt für die Veröffentlichung von Project Loom zum derzeitigen Zeitpunkt noch nicht absehbar.
Im Folgenden sind die betroffenen Klassen aufgelistet:\parencite{Oracle2021-Loom}

\begin{itemize}
  \item java.net.Socket
  \item java.net.ServerSocket
  \item java.net.DatagramSocket/MulticastSocket
  \item java.nio.channels.SocketChannel
  \item java.nio.channels.ServerSocketChannel
  \item java.nio.channels.DatagramChannel
  \item java.nio.channels.Pipe.SourceChannel
  \item java.nio.channels.Pipe.SinkChannel
  \item java.net.InetAddress
\end{itemize}