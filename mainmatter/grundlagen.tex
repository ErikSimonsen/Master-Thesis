% !TEX root = main.tex

\section{Grundlagen}
\label{section:grundlagen}

\subsection{Threads \& Prozesse in Java}
\label{sections:treads_prozesse}
In der Java-Laufzeitumgebung sind Prozesse und Threads als Betriebssystem-Prozesse realisiert, sog. \textit{native threads} oder auch \textit{Kernelthreads}.
Hierbei wird die Ausführungsreihenfolge, die Ausführungszeit und der Prozess- \& Threadwechsel
vom Scheduler \& Dispatcher des Betriebssystems übernommen \parencite{Tanenbaum2016}.
Die Threading-Abstraktion in Java bietet Entwicklern verhältnismäßig leichten Zugriff auf parallelle Programmierung und Synchronisation von Threads.\newline
Servlet-Container binden üblicherweise jede vom Webserver weitergeleitete Anfrage an einen
Thread\footnote{Aus einem, im vornherein erzeugten, Thread-Pool} im Servlet-API, welcher die jeweilige Anfrage imperativ abarbeitet
(daher auch \textit{worker thread}).\newline
Der auszuführende Code ist in diesem Ansatz an den jeweiligen Thread gekoppelt, dieser wartet bei
asynchronen Ereignissen solange, bis er eine Antwort erhält und blockiert den Thread bis dahin.

Um jede Anfrage, und somit jeden Thread, scheinbar parallel zu bearbeiten wird vom Scheduler
des Betriebssystems regelmäßig ein Kontextwechsel zwischen den Threads,
ein \textit{thread-switch}, durchgeführt. Während bei einem Prozesswechsel der gesamte Programmkontext (Adressraum, Inhalt der CPU-Register,
Seitentabelle, geöffnete Dateien und Metainformationen)
gewechselt werden muss, wird bei einem Threadwechsel lediglich der Inhalt der CPU-Register (inkl. Programmzähler) ersetzt\parencite{Brosenne2021}\parencite{Mosberger2002}.
Da der Kontextwechsel, im Fall von \textit{native threads}, durch Systemaufrufe, also vom Kernel des Betriebssystems, ausgeführt werden muss, entsteht auch
bei einem Threadwechsel ein messbarer Zeitverlust.\newline
Weitere Threadwechsel entstehen, wenn ein Thread die zugewiesene Rechenzeit nicht nutzen kann, da er noch durch ein asynchrones Ereignis
(abgesetzte Datenbankabfragen oder weitere aufgerufene Webservices) blockiert, und diese einem anderen Thread zugeteilt wird.

Während dieser Zeitverlust für hoch frequentierte Anwendungen lange Zeit kein Problem darstellte,
sind die Anforderungen an Webanwendungen in den letzten Jahren durch steigende Nutzerzahlen und Architekturen,
die stark auf Client-Server-Kommunikation basieren, erheblich gestiegen.
Ab einer gewissen Menge an Anfragen stellen die Kosten der Threadwechsel von \textit{native threads} ein Performance Bottleneck
(in Form von Durchsatz) dar.

\subsection{Reaktive Programmierung}
\label{section:reaktive_programmierung}
Reaktive Programmierung ist ein Programmierparadigma, bei dem der Programmablauf als Sequenz von asynchronen Ereignissen (Events), und
Daten als -von außen- unveränderliche (immutable) Datenströme (Streams) dargestellt werden.
Sobald es innerhalb des Datenstroms zu Änderungen kommt werden diese als Events durch einen Publisher veröffentlicht.\footnote{Änderungen in Datenströmen
    sind der quasi der Stimulus}
Die eigentliche Programmlogik wird in Funktionen ausgeführt, die auf die veröffentlichten Events hören (Subscriber), sie verarbeiten und wiederrum welche
veröffentlichen können. Die Grundidee orientiert sich am Observer-Pattern und dessen Ausprägung: dem Publish-Subscribe Pattern, erweitert diese aber
noch um die Benachrichtigungen des Subscribers:
\begin{enumerate}
    \item Sobald keine Events mehr kommen
    \item Wenn ein Fehler aufgetreten ist
\end{enumerate}
Indem Änderungen direkt propagiert werden und Subscriber keine Kontrolle über den Datenfluss haben, sondern lediglich über Änderungen informiert werden,
können Programme ohne jeglichen Zustand realisiert werden\parencite{Escoffier2017}.


Reaktive Programmierung verinnerlicht das Konzept von nicht-blockierender bzw. asynchroner Ein- und Ausgabe (I/O).
Dabei wird, statt wie bei synchroner bzw. blockierender Ein- und Ausgabe die restliche Ausführung des Programms
zu blockieren bis die Datenübertragung abgeschlossen ist, nach dem Start der Übertragung bereits begonnen die Teile des
Programms auszuführen, die nicht von dem Ergebnis der I/O Operation abhängen.

Dadurch können mehrere parallele Anfragen von dem gleichen Thread bearbeitet werden.
Methoden die blockierende I/O Operationen ausführen, wie Datenbankzugriffe oder Anfragen von externen Services,
geben beim Aufruf unverzüglich einen Publisher zurück, auf dem sich der Aufrufer registriert (subscribe).
Dadurch wird der bearbeitende Thread nicht blockiert, und kann die nächste Anfrage bearbeiten.
Sobald das Ergebnis der I/O Operation bereit ist, wird es dem Publisher in Form eines Events mitgeteilt, von diesem veröffentlicht und die Anfrage kann
vom Aufrufer bzw. Subscriber weiter bearbeitet werden.

Da durch dieses Modell der auszuführende Code nicht mehr an den jeweils ausführenden Thread gebunden wird, erlaubt es die Nutzung eines einzigen
Threads (\textit{sog. IO Thread}) statt eines \textit{Threadpools}.
Dadurch ergeben sich folgende Vorteile:

\begin{enumerate}
    \item Die Antwortenzeiten sind für eine hohe Last geringer, da deutlich weniger Threadwechsel gemacht werden müssen
    \item Der Speicherverbrauch ist geringer, da weniger Threads genutzt werden
    \item Der Grad der Paralellität ist nicht von der Anzahl der Threads begrenzt
\end{enumerate}

Allerdings gibt es auch einige gravierende Nachteile:

\begin{enumerate}
    \item Asynchroner Code ist schwieriger zu schreiben, lesen, testen und zu debuggen als imperativer Code
    \item Sehr aufwendig in bestehende klassische Anwendungen zu integrieren
    \item Reaktive Anwendungen müssen in jeder Schicht reaktiv sein (Transaktionen, Security, Datenbanktreiber)
    \item Da nur ein IO-Thread genutzt wird resultieren blockierende I/O-Operationen in der Blockierung der gesamten Anwendung
\end{enumerate}

Ein beliebtes Threading-Modell für die Verarbeitung von asynchronen Events ist die Event Loop. Sobald ein Event entsteht wird es einer Warteschlange (Queue)
in der Event Loop hinzugefügt. Solange der Main-Thread aktiv ist und die Queue noch Events enthält, wird in einer Schleife das nächste Event
abgerufen und an den, für diesen Eventtyp, registrierten Event-Handler bzw. oben beschriebenen Subscriber weitergeleitet.
Dies können beispielsweise I/O-Events sein, die signalisieren das Daten bereit zur Weiterverarbeitung sind, aber auch jegliches andere Event.
Die Event Loop wird in der Regel vom Main-Thread (in diesem Fall dem genannten IO-Thread) ausgeführt, daher darf das Verarbeiten von Events
keine blockierenden, oder zeitintensiven Operationen beinhalten\parencite{Ponge2020}.

\subsubsection{Alternativen}
In Java 1.1 wurden Threads als sog. \textit{Green Threads} implementiert. Dabei wurde die Möglichkeit Threads vom Betriebssystem verwalten zu lassen 
gar nicht genutzt. Stattdessen lief die komplette JVM in einem einzigen Prozess. 
\textit{Green threads} waren als \textit{user threads} implementiert \footnote{Auch als \textit{Fiber} oder \textit{virtual thread} bezeichnet},
 dabei ist die Funktionalität
nicht im Kernel implementiert (wie bei \textit{kernel-/native threads}), sondern in einer Programmbibliothek im \textit{Userspace}.
Da sich das Betriebssystem nicht um das Scheduling von \textit{user threads} kümmert, wurde dies über einen eigenen Scheduling-Algorithmus der JVM
geregelt.
Ein \textit{green thread} existiert lediglich als Objekt innerhalb der JVM, und durch die virtuelle Speicherverwaltung entfallen somit
 die aufwändigen Betriebssystemaufrufe beim 
Erstellen eines Threads, sowie bei Thread- bzw. Kontextwechseln, denn der ausführende Main-Thread bleibt gleich.
\footnote{Jeder Prozess hat mind. einen Thread (kernel-thread) der das Programm ausführt. Dieser wird daher auch als \textit{main-thread} bezeichnet.}

Die Threadwechsel der \textit{user-threads} erfolgten ausschließlich innerhalb des Main-Threads, weswegen keine echte Parallelität realisiert werden konnte, da immer nur ein 
Prozessorkern genutzt wurde. 
Während der Vorteil dieses Modells darin lag, dass es keine 'echten' parallelen Zugriffe auf eine Resource innerhalb des JVM-Prozesses geben konnte und die Synchronisation
von Datenzugriffen daher leicht war, überwog schließlich der Umstand, dass keine Nutzung von mehreren Prozessorkernen durch Multithreading möglich war
, weswegen \textit{Green Threads} ab Java 1.3 zugunsten von \textit{native threads} entfernt wurden.

Mit dem OpenJDK Projekt \textit{Project Loom} ist die Idee von \textit{Green threads}
 wieder aufgegriffen worden, allerdings nun als Ergänzung (statt Alternative) zu \textit{native threads}. 
Statt alle virtuellen Threads auf dem nativen Main-Thread auszuführen, werden diese von einer geringen Anzahl an nativen \textit{worker threads},
die als Carrier eingesetzt werden, ausgeführt.
Deren Anzahl ist so gewählt, dass alle CPU Kerne durch Multithreading dauerhaft benutzt werden können 
\footnote{In der Praxis laufen natürlich noch andere Prozesse auf dem Server, deren Threads auch ausgeführt werden müssen.}
, aber so wenig Kontextwechsel wie möglich ausgeführt werden müssen. \footnote{Im Idealfall würde auf jedem CPU Kern ein \textit{worker thread} laufen, ohne jemals
einen Thread- bzw. Kontextwechsel machen zu müssen.}
Während ein nativer Thread in einer 64 Bit JVM ca. 1 MB für den Threadstack reserviert und zusätzlich noch Metadaten abspeichert, ist ein virtueller Thread 
lediglich ein Objekt im virtuellen Speicher der JVM und benötigt darüber hinaus für sich selber extrem wenig Resourcen (da er ja im Hintergrund von einem 
nativen Thread abgearbeitet wird). 
Aus diesem Grund können durchaus mehrere Millionen virtueller Threads erzeugt werden (bei entsprechendem allokierten Heap-Speicher der JVM), wohingegen
die Erstellung von 10.000 nativen Threads entweder den allokierten Speicher weit überschreitet (wodurch der JVM-Prozess abstürzt) oder die Threadgrenze
des Betriebssystems wird überschritten.

Sobald ein virtueller Thread nun eine blockierende I/O Operation ausführt signalisiert er dem darunterliegenden nativen Thread, dass er momentan nichts machen kann 
außer Warten, und erlaubt dem nativen Thread zu einem anderen virtuellen Thread zu wechseln.

Das große Versprechen des Projektes ist außerdem, das Entwickler keine asynchronen Programmierparadigmen (wie u.A. \textit{Reactive Programming})
 nutzen müssen um die beschriebenen virtuellen Threads (und die damit einhergehenden wesentlichen Performanceverbesserungen) nutzen zu können.
 Um dieses Versprechen zu halten werden virtuelle Threads, statt als Bibliothek eines Drittanbieters, in Form einer eigenen JDK Version bereitgestellt. 
 In dieser Version wurden viele Teile der Standardbibliothek die mit I/O-Operationen arbeiten so angepasst, dass virtuelle Threads statt native Threads 
 genutzt werden. Auf diese Weise können I/O-Operationen, wie beispielsweise der Aufruf einer Netzwerkfunktionalität, ohne Änderungen am Programm
 die virtuellen Threads nutzen und blockieren den darunterliegenden nativen Thread nicht mehr.

\url{https://jaxenter.de/reactive-programming-82051}
\url{https://jaxenter.de/java/project-loom-besser-skalieren-durch-virtuelle-threads-2-94424}
\url{https://blogs.oracle.com/javamagazine/going-inside-javas-project-loom-and-virtual-threads}
\url{https://inside.java/2021/05/10/networking-io-with-virtual-threads/}

\parencite{Oracle2021}
\subsection{Reaktive Datenströme}
\label{section.reaktive_datenströme}
\url{http://www.reactive-streams.org/}

\subsection{Reaktive Systeme}
\label{section:reaktive_systeme}
\url{https://www.reactivemanifesto.org/}

\subsubsection{Eigenschaften}

\subsubsection{Anwendungsgebiete}

\subsection{Werkzeuge}
\subsubsection{Java Ökosystem}
// CompletableFuture
// Reactive streams
// Flow Api
// JavaRx, Vert.x \& Mutiny in Quarkus
\subsubsection{Andere}

